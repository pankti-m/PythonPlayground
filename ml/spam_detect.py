import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import string
import nltk
from nltk.corpus import stopwords
from wordcloud import WordCloud
nltk.download('stopwords')

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from keras.callbacks import EarlyStopping, ReduceLROnPlateau

import warnings
warnings.filterwarnings('ignore')

data = pd.read_csv('spam_ham_dataset.csv')
data.head()
print("===== Data Head Is: ======")
print(data)

sns.countplot(x='label', data=data)
plt.show()

## Number of Ham Samples >> Spam.  So downsample the Ham Class
ham_msg = data[data['label'] == 'ham']
spam_msg = data[data['label'] == 'spam']

ham_msg_balanced = ham_msg.sample(n=len(spam_msg), random_state=42)

# Combine balanced data
balanced_data = pd.concat([ham_msg_balanced, spam_msg]).reset_index(drop=True)

sns.countplot(x='label', data=balanced_data)
plt.title('Balanced Distribution of Ham and Spam Emails')
plt.xticks(ticks=[0, 1], labels=['Ham (Not Spam)', 'Spam'])
plt.show()

## Clean the data
balanced_data['text'] = balanced_data['text'].str.replace('Subject', '')
balanced_data.head()

#1. Remove Punctuations
def remove_punctuations(text):
    temp = str.maketrans('', '', punctuations_list)
    return text.translate(temp)
punctuations_list = string.punctuation
balanced_data['text'] = balanced_data['text'].apply(lambda x:remove_punctuations(x))
print(balanced_data)

#2. Remove StopWords
def remove_stopwords(text):
    stop_words = stopwords.words('english')
    imp_words = []
    for word in str(text).split():
        word = word.lower()
        if (word not in stop_words):
            imp_words.append(word)
    output = " ".join(imp_words)
    return output
balanced_data['text'] = balanced_data['text'].apply(lambda x:remove_stopwords(x))
print(balanced_data.head())

train_X, test_X, train_Y, test_Y = train_test_split(balanced_data['text'], balanced_data['label'], test_size=0.2, random_state=42)
tokenizer = Tokenizer()
tokenizer.fit_on_texts(train_X)

train_sequences = tokenizer.texts_to_sequences(train_X)
test_sequences = tokenizer.texts_to_sequences(test_X)

# Define Max Sequence Length
max_len = 100
train_sequences = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')
test_sequences = pad_sequences(test_sequences, maxlen = max_len, padding='post', truncating='post')

train_Y = (train_Y == 'spam').astype(int)
test_Y = (test_Y == 'spam').astype(int)

# Build the model
model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=32,
                              input_shape=(max_len,)),
    tf.keras.layers.LSTM(16),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
    ])

model.compile(
        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
        optimizer='adam',
        metrics=['accuracy']
        )

model.summary()

# Train the Model
es = EarlyStopping(patience=3, monitor='val_accuracy', restore_best_weights=True)
lr = ReduceLROnPlateau(patience=2, monitor='val_loss', factor=0.5, verbose=0)

history = model.fit(
        train_sequences, train_Y,
        validation_data=(test_sequences, test_Y),
        epochs=20,
        batch_size=32,
        callbacks=[lr, es]
        )
test_loss, test_accuracy = model.evaluate(test_sequences, test_Y)
print('Test Loss:', test_loss)
print('Test Accuracy:', test_accuracy)
